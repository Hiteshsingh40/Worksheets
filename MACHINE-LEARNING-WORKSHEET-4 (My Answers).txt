                                                   MACHINE LEARNING – WORKSHEET 4

Ans1. (A)GridSearchCV() 

Ans2. (A)Random forest 

Ans3. (B)The regularization will decrease

Ans4. (A)It regularizes the decision tree by limiting the maximum depth up to which a tree can be grown.

Ans5. (A)It's an ensemble of weak learners.

Ans6. (C)Both of them

Ans7. (B)Bias will decrease, Variance increase

Ans8. (C)model is performing good 


Ans10. Random forests are a strong modeling technique and much more robust than a single decision tree. They aggregate many decision trees to limit overfitting as well as error due to bias and therefore yield useful results.
Random forests consist of multiple single trees each based on a random sample of the training data. They are typically more accurate than single decision trees.
Two reasons why random forests outperform single decision trees.
1. Higher resolution in the feature space
Trees are unpruned. While a single decision tree like CART is often pruned, a random forest tree is fully grown and unpruned, and so, naturally, the feature space is split into more and smaller regions.
Trees are diverse. Each random forest tree is learned on a random sample, and at each node, a random set of features are considered for splitting. Both mechanisms create diversity among the trees.
2. Handling Overfitting
A single decision tree needs pruning to avoid overfitting. The following shows the decision boundary from an unpruned tree. The boundary is smoother but makes obvious mistakes (overfitting).

Ans11. Scaling, Standardizing and Transformation are important steps of numeric feature engineering and they are being used to treat skewed features and rescale them for modelling. Machine Learning & Deep Learning algorithms are highly dependent on the input data quality. If Data quality is not good, even high-performance algorithms are of no use. It’s as simple as Garbage In : Garbage Out.
Scaling is required to rescale the data and it’s used when we want features to be compared on the same scale for our algorithm. And, when all features are in the same scale, it also helps algorithms to understand the relative relationship better.
If dependent features are transformed to normality, Scaling should be applied after transformation.
Which Algorithms may benefit after Scaling? Scaling is helpful in Distance-based algorithms and also in faster convergence
Linear & Logistic Regression, KMeans/ KNN, Neural Networks, PCA will benefit from scaling
Which Algorithms may not benefit after Scaling? Some algorithms are independent of Scaling. Entropy & Information Gain based techniques are not sensitive to monotonic transformation.
Tree-Based Algorithms, Decision Tree, Random Forest, Boosted Trees(GBM, light GBM, xgboost) may not benefit from scaling.
Scaler model fitted on the train data will be used to transform the test set. Never fit scaler again on the test data.
Sklearn has following four scalers primarily
1. Minmax scaler
2. Robust scaler
3. Standard Scaler
4. Normalizer.
Minmax scaler :- Minmax scaler should be the first choice for scaling. For each feature, each value is subtracted by the minimum value of the respective feature and then divide by the range of original maximum and minimum of the same feature. It has a default range between [0,1].
RobustScaler:- RobustScaler can be used when data has high outliers and we want to subside their effects. But unimportant outliers should be removed in the first place. RobustScaler subtracts the column’s median and divides by the interquartile range.
StandardScaler:- StandardScaler rescales each column to have 0 mean and 1 Standard Deviation. It standardizes a feature by subtracting the mean and dividing by the standard deviation. If the original distribution is not normally distributed, it may distort the relative space among the features.

Ans12. Optimization refers to the task of minimizing/maximizing an objective function f(x) parameterized by x. In machine/deep learning terminology, it’s the task of minimizing the cost/loss function J(w) parameterized by the model’s parameters w ∈ R^d.
Gradient Descent is the most common optimization algorithm in machine learning and deep learning. It is a first-order optimization algorithm. This means it only takes into account the first derivative when performing the updates on the parameters. On each iteration, we update the parameters in the opposite direction of the gradient of the objective function J(w) w.r.t the parameters where the gradient gives the direction of the steepest ascent. The size of the step we take on each iteration to reach the local minimum is determined by the learning rate α. Therefore, we follow the direction of the slope downhill until we reach a local minimum.

Ans13.Classification accuracy is the most-used metric for evaluating classification models.
The reason for its wide use is because it is easy to calculate, easy to interpret, and is a single number to summarize the model’s capability.
As such, it is natural to use it on imbalanced classification problems, where the distribution of examples in the training dataset across the classes is not equal.
When the class distribution is slightly skewed, accuracy can still be a useful metric. When the skew in the class distributions are severe, accuracy can become an unreliable measure of model performance.
classification accuracy is almost universally inappropriate for imbalanced classification. The reason is, a high accuracy (or low error) is achievable by a no skill model that only predicts the majority class.

Ans14. The F-score, also called the F1-score, is a measure of a model’s accuracy on a dataset. It is used to evaluate binary classification systems, which classify examples into ‘positive’ or ‘negative’.
The F-score is a way of combining the precision and recall of the model, and it is defined as the harmonic mean of the model’s precision and recall.
The F-score is commonly used for evaluating information retrieval systems such as search engines, and also for many kinds of machine learning models, in particular in natural language processing.

F-score Formula
The formula for the standard F1-score is the harmonic mean of the precision and recall. A perfect model has an F-score of 1.
F1= 2*precision *recall/ precision+ recall

Ans15. Most scikit-learn objects are either transformers or models.
Transformers are for pre-processing before modeling. The Imputer class (like SimpleImputer for filling in missing values) and FeatureSelection classes in sklearn are an example of some transformers.
Models are used to make predictions like Linear Regression model, Decision Tree model, Random Forest model etc. You will usually pre-process your data (with transformers) before putting it in a model.
Now the usage of methods fit(), transform(), fit_transform() and predict() depend on the type of object.

For Transformers:
1. fit() - It is used for calculating the initial filling of parameters on the training data (like mean of the column values) and saves them as an internal objects state.
2. transform() - Use the above calculated values and return modified training data
3. fit_transform() - It joins above two steps. Internally, it just calls first fit() and then transform() on the same data.

For Models:
1. fit() - It calculates the parameters/weights on training data (e.g. parameters returned by coef() in case of Linear Regression) and saves them as an internal objects state.
2. predict() - Use the above calculated weights on test data to make the predictions
3. transform() - Cannot be used
4. fit_transform() - Cannot be used